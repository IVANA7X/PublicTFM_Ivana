#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Script para realizar fine-tuning de un modelo CLIP con Sentence Transformers
para un dataset de pares de imágenes y texto, con evaluación específica por descriptor.
"""

import os
import pandas as pd
import torch
from PIL import Image
from tqdm.auto import tqdm
from datasets import DatasetDict, Dataset
from sentence_transformers import SentenceTransformer, models, losses, InputExample, evaluation, util # util añadido
from torch.utils.data import DataLoader
import wandb

# --- Configuración ---
BATCH_SIZE = 32
NUM_EPOCHS = 20 # Puedes ajustar esto según tus experimentos
MODEL_NAME = "clip-ViT-B-32"
LEARNING_RATE = 1e-5 # Configura tu learning rate deseado (ej. 2e-5, 1e-5, 3e-5)

# NUEVO: Ruta base donde se encuentran tus imágenes (cuyos nombres están en la columna image_path)
IMAGE_BASE_PATH = "/content/dataset_finetuning/content/drive/MyDrive/TFM/dataset_finetuning" # ¡¡AJUSTA ESTA RUTA A TU CASO!!

# NUEVO: Ruta para guardar este nuevo modelo fine-tuneado
OUTPUT_PATH_NEW = "./sentence-transformer-finetuned-model-cat"


# Nombres de los archivos CSV (actualizados)
TRAIN_CSV = '/content/drive/MyDrive/TFM/dataset/train_cat.csv'
VAL_CSV = '/content/drive/MyDrive/TFM/dataset/val_cat.csv'
TEST_CSV = '/content/drive/MyDrive/TFM/dataset/test_cat.csv'


def load_image(path):
    """Carga una imagen desde la ruta especificada."""
    try:
        return Image.open(path).convert('RGB')
    except Exception as e:
        print(f"Error al cargar la imagen {path}: {e}")
        return Image.new('RGB', (224, 224), color=0)

def load_datasets(train_csv, val_csv, test_csv):
    """Carga los datasets desde archivos CSV."""
    print("Cargando datasets...")
    try:
        # CORREGIDO: Leer los dataframes desde los CSVs proporcionados
        df_train = pd.read_csv(train_csv)
        df_val = pd.read_csv(val_csv)
        df_test = pd.read_csv(test_csv)
    except FileNotFoundError as e:
        print(f"Error: No se pudo encontrar uno de los archivos CSV: {e}")
        raise
    except Exception as e:
        print(f"Error cargando los CSVs: {e}")
        raise

    # Convertir a datasets de Hugging Face
    train_ds = Dataset.from_pandas(df_train)
    val_ds = Dataset.from_pandas(df_val)
    test_ds = Dataset.from_pandas(df_test)

    dataset_dict = DatasetDict({
        'train': train_ds,
        'valid': val_ds,
        'test': test_ds
    })

    print(f"Dataset de entrenamiento: {len(dataset_dict['train'])} ejemplos")
    print(f"Dataset de validación: {len(dataset_dict['valid'])} ejemplos")
    print(f"Dataset de prueba: {len(dataset_dict['test'])} ejemplos")
    return dataset_dict

# MODIFICADO: Añadir image_base_path
def create_training_examples(dataset, image_base_path):
    """Crea ejemplos para el entrenamiento a partir de un dataset."""
    examples = []
    for item in tqdm(dataset, desc="Creando ejemplos de entrenamiento/evaluación"):
        image_filename = item['image_path'] # Ahora es solo el nombre del archivo
        # MODIFICADO: Construir ruta completa
        full_image_path = os.path.join(image_base_path, image_filename)

        caption = item['caption']
        similarity = float(item['similarity'])

        if not os.path.exists(full_image_path):
            print(f"Advertencia: La imagen {full_image_path} no existe. Saltando ejemplo.")
            continue

        # Pasamos la ruta completa a InputExample para que SBERT la maneje si es necesario
        examples.append(InputExample(texts=[caption, full_image_path], label=similarity))
    return examples

# MODIFICADO: Añadir image_base_path
def prepare_retrieval_eval_data(dataset, image_base_path):
    """Prepara los datos para InformationRetrievalEvaluator."""
    queries = {}
    corpus = {}
    relevant_docs = {}

    # Necesitamos asegurar que los IDs de corpus sean únicos y mapeen a las imágenes.
    # También, si una imagen aparece múltiples veces con diferentes captions positivas,
    # el corpus solo debe tenerla una vez.

    # Primero, construir el corpus con image_filenames y mapear filenames a corpus_ids
    corpus_id_counter = 0
    filename_to_corpus_id = {}

    # Usaremos el dataframe subyacente para acceder a todas las columnas necesarias, incluyendo 'descriptor'
    df_dataset = dataset.to_pandas()

    for index, item in df_dataset.iterrows():
        image_filename = item['image_path']
        # MODIFICADO: Construir ruta completa
        full_image_path = os.path.join(image_base_path, image_filename)

        if full_image_path not in filename_to_corpus_id: # Añadir cada imagen al corpus solo una vez
            current_corpus_id = str(corpus_id_counter)
            filename_to_corpus_id[full_image_path] = current_corpus_id
            corpus[current_corpus_id] = full_image_path # SBERT espera la ruta para codificarla
            corpus_id_counter += 1

    # Ahora, construir queries y relevant_docs
    query_id_counter = 0
    for index, item in df_dataset.iterrows():
        if float(item['similarity']) >= 1.0: # Solo pares positivos para queries y docs relevantes
            caption = item['caption']
            image_filename = item['image_path']
            # MODIFICADO: Construir ruta completa
            full_image_path = os.path.join(image_base_path, image_filename)

            current_query_id = str(query_id_counter)
            queries[current_query_id] = caption

            # Obtener el corpus_id de la imagen relevante
            if full_image_path in filename_to_corpus_id:
                relevant_corpus_id = filename_to_corpus_id[full_image_path]
                if current_query_id not in relevant_docs:
                    relevant_docs[current_query_id] = set()
                relevant_docs[current_query_id].add(relevant_corpus_id)
            query_id_counter += 1

    print(f"Preparado {len(queries)} queries y {len(corpus)} items de corpus para InformationRetrievalEvaluator.")
    if not queries or not corpus or not relevant_docs:
        print("ADVERTENCIA: Queries, corpus o relevant_docs están vacíos. La evaluación de recuperación podría no funcionar.")
    return queries, corpus, relevant_docs


def train_model(model, train_examples, dev_evaluator, num_epochs, learning_rate, output_path): # Añadido learning_rate y output_path
    """Entrena el modelo CLIP con Sentence Transformers."""
    print(f"Configurando entrenamiento con LR: {learning_rate}...")
    train_loss = losses.CosineSimilarityLoss(model)
    warmup_steps = int(len(train_examples) * num_epochs * 0.1)
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)

    print(f"Iniciando entrenamiento por {num_epochs} épocas...")
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        evaluator=dev_evaluator,
        epochs=num_epochs,
        warmup_steps=warmup_steps,
        optimizer_params={'lr': learning_rate}, # MODIFICADO: Usar learning_rate
        output_path=output_path,               # MODIFICADO: Usar output_path
        show_progress_bar=True,
        evaluation_steps=int(len(train_dataloader) * 0.1)
    )
    print(f"Entrenamiento completado. Modelo guardado en {output_path}")
    return model

# La función evaluate_model actual usa EmbeddingSimilarityEvaluator.
# La evaluación de InformationRetrieval se hace con dev_evaluator y test_evaluator.
# Esta función se puede mantener si se quiere esa métrica de similitud específica.
def evaluate_embedding_similarity(model, examples_for_eval, name='eval'):
    print(f"Evaluando similitud de embeddings en el conjunto '{name}'...")
    evaluator = evaluation.EmbeddingSimilarityEvaluator(
        sentences1=[example.texts[0] for example in examples_for_eval],
        sentences2=[example.texts[1] for example in examples_for_eval], # Son rutas completas
        scores=[example.label for example in examples_for_eval],
        name=name
    )
    score = evaluator(model)
    if isinstance(score, dict):
        for metric_name, metric_value in score.items():
            print(f"  {name}_{metric_name}: {metric_value:.4f}")
            if wandb.run: wandb.summary[f"{name}_{metric_name}"] = metric_value
    else:
        print(f"  Puntuación de similitud en '{name}': {score:.4f}")
        if wandb.run: wandb.summary[f"{name}_similarity_score"] = score
    return score


# --- NUEVA FUNCIÓN PARA RECALL@K ESPECIALIZADO POR DESCRIPTOR ---
def evaluate_descriptor_specific_recall(model, val_df_pandas, image_base_path, k_values=[1, 3, 5, 10, 20]):
    """
    Calcula Recall@k para cada descriptor en el conjunto de validación.
    val_df_pandas: DataFrame de Pandas del conjunto de validación, con columnas
                   'image_path', 'caption', 'similarity', 'descriptor'.
    """
    print("\n--- Iniciando Evaluación de Recall@k Específico por Descriptor ---")

    # 1. Pre-cómputo de Embeddings
    print("Pre-calculando embeddings para queries y corpus de validación...")

    # Corpus: Imágenes únicas con su descriptor
    corpus_image_data = []
    # Agrupar por 'image_path' para tomar el primer descriptor si hay duplicados de filename (no debería si el split se hizo bien en base única)
    for filename, group in val_df_pandas.groupby('image_path'):
        descriptor = group['descriptor'].iloc[0] # Tomar el descriptor de la primera aparición
        full_path = os.path.join(image_base_path, filename)
        corpus_image_data.append({
            'filename': filename,
            'descriptor': descriptor,
            'full_path': full_path
        })

    all_corpus_full_paths = [item['full_path'] for item in corpus_image_data]
    if not all_corpus_full_paths:
        print("Corpus de validación vacío para evaluación por descriptor. Saltando.")
        return {}

    all_corpus_embeddings = model.encode(all_corpus_full_paths, batch_size=BATCH_SIZE, convert_to_tensor=True, show_progress_bar=True)
    for i, item in enumerate(corpus_image_data):
        item['embedding'] = all_corpus_embeddings[i]

    # Queries: Captions de pares positivos con su descriptor y la imagen relevante
    queries_data = []
    df_val_positive = val_df_pandas[val_df_pandas['similarity'] == 1.0]
    for index, row in df_val_positive.iterrows():
        queries_data.append({
            'text': row['caption'],
            'descriptor': row['descriptor'],
            'relevant_image_filename': row['image_path']
        })

    all_query_texts = [item['text'] for item in queries_data]
    if not all_query_texts:
        print("No hay queries positivas en el conjunto de validación para evaluación por descriptor. Saltando.")
        return {}

    all_query_embeddings = model.encode(all_query_texts, batch_size=BATCH_SIZE, convert_to_tensor=True, show_progress_bar=True)
    for i, item in enumerate(queries_data):
        item['embedding'] = all_query_embeddings[i]

    print(f"Embeddings calculados: {len(corpus_image_data)} imágenes en corpus, {len(queries_data)} queries.")

    # 2. Calcular Recall@k por descriptor
    recall_hits_per_descriptor = {desc: {k: 0 for k in k_values} for desc in val_df_pandas['descriptor'].unique()}
    query_counts_per_descriptor = {desc: 0 for desc in val_df_pandas['descriptor'].unique()}

    for query_item in tqdm(queries_data, desc="Evaluando queries por descriptor"):
        query_embedding = query_item['embedding']
        query_descriptor = query_item['descriptor']
        true_relevant_filename = query_item['relevant_image_filename']

        query_counts_per_descriptor[query_descriptor] += 1

        # Filtrar corpus para el descriptor actual
        current_descriptor_corpus_filtered = [] # Lista de {'filename': ..., 'embedding': ...}
        for img_data in corpus_image_data:
            if img_data['descriptor'] == query_descriptor:
                current_descriptor_corpus_filtered.append(img_data)

        if not current_descriptor_corpus_filtered:
            # print(f"  Advertencia: No hay imágenes en corpus para descriptor '{query_descriptor}' (Query: '{query_item['text'][:30]}...')")
            continue

        current_corpus_embeddings_tensor = torch.stack([item['embedding'] for item in current_descriptor_corpus_filtered])
        current_corpus_filenames = [item['filename'] for item in current_descriptor_corpus_filtered]

        similarities = util.cos_sim(query_embedding.unsqueeze(0), current_corpus_embeddings_tensor)[0]
        ranked_indices = torch.argsort(similarities, descending=True)

        for k in k_values:
            top_k_filenames = [current_corpus_filenames[idx.item()] for idx in ranked_indices[:k]]
            if true_relevant_filename in top_k_filenames:
                recall_hits_per_descriptor[query_descriptor][k] += 1

    # 3. Consolidar y mostrar resultados
    final_recall_scores = {}
    print("\nResultados de Recall@k Específico por Descriptor (Validation):")
    for descriptor, k_hits_map in recall_hits_per_descriptor.items():
        final_recall_scores[descriptor] = {}
        if query_counts_per_descriptor.get(descriptor, 0) > 0:
            print(f"  Descriptor: {descriptor} (Consultas: {query_counts_per_descriptor[descriptor]})")
            for k, hits in k_hits_map.items():
                recall = hits / query_counts_per_descriptor[descriptor]
                final_recall_scores[descriptor][f"Recall@{k}"] = recall
                print(f"    Recall@{k}: {recall:.4f}")
                if wandb.run: wandb.summary[f"val_recall_at_{k}_desc_{descriptor.replace(' ', '_')}"] = recall
        else:
            print(f"  Descriptor: {descriptor} (Consultas: 0)")


    print("--- Fin de Evaluación de Recall@k Específico por Descriptor ---")
    return final_recall_scores


def calculate_similarity(model, image_filename, text, image_base_path): # MODIFICADO
    """Calcula la similitud entre una imagen (por filename) y un texto."""
    # MODIFICADO: Construir ruta completa
    full_image_path = os.path.join(image_base_path, image_filename)
    try:
        embeddings = model.encode([text, full_image_path], convert_to_tensor=True)
        text_embedding = embeddings[0].reshape(1, -1)
        image_embedding = embeddings[1].reshape(1, -1)
        similarity = util.pytorch_cos_sim(text_embedding, image_embedding)[0][0].item()
        return similarity
    except Exception as e:
        print(f"Error en calculate_similarity para {full_image_path} y texto '{text}': {e}")
        return 0.0 # O manejar el error como prefieras

def create_sentence_transformer_model(model_name_or_path): # MODIFICADO para aceptar ruta
    """Carga un modelo SentenceTransformer desde un nombre o ruta."""
    print(f"Cargando modelo: {model_name_or_path}...")
    model = SentenceTransformer(model_name_or_path)
    return model

def main():
    """Función principal."""
    # Usar el nuevo OUTPUT_PATH y LEARNING_RATE
#    wandb.tensorboard.patch(root_logdir=OUTPUT_PATH_NEW)
    wandb.init(
        project="clip-finetuning-sangre-memoria", # Nuevo nombre de proyecto
        config={
            "model_name": MODEL_NAME,
            "batch_size": BATCH_SIZE,
            "num_epochs": NUM_EPOCHS,
            "learning_rate": LEARNING_RATE, # AÑADIDO LEARNING_RATE
            "loss_function": "MultipleNegativesRankingLoss",
            "output_path": OUTPUT_PATH_NEW, # Usar nueva ruta
            "train_csv": TRAIN_CSV,
            "val_csv": VAL_CSV,
            "image_base_path": IMAGE_BASE_PATH
        }
    )

    # Rutas a los archivos CSV actualizadas
    train_csv_path = TRAIN_CSV
    val_csv_path = VAL_CSV
    test_csv_path = TEST_CSV

    os.makedirs(OUTPUT_PATH_NEW, exist_ok=True) # Usar nueva ruta

    dataset_dict = load_datasets(train_csv_path, val_csv_path, test_csv_path)

    # MODIFICADO: Pasar IMAGE_BASE_PATH
    print("Preparando datos para el entrenamiento...")
    train_examples = create_training_examples(dataset_dict['train'], IMAGE_BASE_PATH)
    # val_examples y test_examples para EmbeddingSimilarityEvaluator también necesitan la ruta base
    val_examples_for_emb_sim = create_training_examples(dataset_dict['valid'], IMAGE_BASE_PATH)
    # test_examples_for_emb_sim = create_training_examples(dataset_dict['test'], IMAGE_BASE_PATH)


    print(f"Ejemplos de entrenamiento: {len(train_examples)}")

    # InformationRetrievalEvaluator para validación durante el entrenamiento
    print("\nPreparando datos para InformationRetrievalEvaluator (validación)...")
    # MODIFICADO: Pasar IMAGE_BASE_PATH
    val_queries, val_corpus, val_relevant_docs = prepare_retrieval_eval_data(dataset_dict['valid'], IMAGE_BASE_PATH)

    if val_queries and val_corpus and val_relevant_docs:
        dev_evaluator = evaluation.InformationRetrievalEvaluator(
            queries=val_queries,
            corpus=val_corpus,
            relevant_docs=val_relevant_docs,
            name='validation_retrieval', # Nombre más específico
            precision_recall_at_k=[1, 3, 5, 10, 20],
            show_progress_bar=True # Es útil ver el progreso aquí
        )
        print("InformationRetrievalEvaluator creado para validación durante entrenamiento.")
    else:
        print("No se creará InformationRetrievalEvaluator para validación debido a datos vacíos.")
        dev_evaluator = None


    # Cargar modelo CLIP (base para fine-tuning)
    model = create_sentence_transformer_model(MODEL_NAME)

    # Entrenar modelo
    model = train_model(model, train_examples, dev_evaluator, NUM_EPOCHS, LEARNING_RATE, OUTPUT_PATH_NEW) # Pasar LR y Output

    # --- EVALUACIÓN POST-ENTRENAMIENTO ---
    print("\n--- Iniciando Evaluaciones Post-Entrenamiento ---")
    # Cargar el mejor modelo guardado por el entrenamiento
    print(f"Cargando el mejor modelo desde: {OUTPUT_PATH_NEW}")
    model_fine_tuned = create_sentence_transformer_model(OUTPUT_PATH_NEW)


    # 1. Evaluación de Similitud de Embeddings en Validación (opcional, como antes)
    if val_examples_for_emb_sim:
        print("\nEvaluando Similitud de Embeddings en Validation Set...")
        evaluate_embedding_similarity(model_fine_tuned, val_examples_for_emb_sim, name='val_emb_sim')

    # 2. NUEVO: Evaluación de Recall@k Específico por Descriptor en Validación
    # Pasamos el DataFrame de pandas de validación directamente
    df_val_pandas = dataset_dict['valid'].to_pandas()
    evaluate_descriptor_specific_recall(model_fine_tuned, df_val_pandas, IMAGE_BASE_PATH)


    # 3. Evaluación de Recuperación de Información en Test Set (como antes)
    print("\nPreparando datos para InformationRetrievalEvaluator (test)...")
    # MODIFICADO: Pasar IMAGE_BASE_PATH
    test_queries, test_corpus, test_relevant_docs = prepare_retrieval_eval_data(dataset_dict['test'], IMAGE_BASE_PATH)
    if test_queries and test_corpus and test_relevant_docs:
        test_retrieval_evaluator = evaluation.InformationRetrievalEvaluator(
            queries=test_queries,
            corpus=test_corpus,
            relevant_docs=test_relevant_docs,
            name='test_retrieval',
            precision_recall_at_k=[1, 3, 5, 10, 20],
            show_progress_bar=True
        )
        print("\nEvaluando Recuperación de Información en Test Set...")
        test_retrieval_evaluator(model_fine_tuned) # Los resultados se imprimen y se envían a wandb si está configurado en el evaluador
    else:
        print("No se realizará la evaluación de recuperación en Test Set debido a datos vacíos.")

    # Ejemplo de uso (actualizado para usar IMAGE_BASE_PATH)
    if len(dataset_dict['test']) > 0:
        example_image_filename = dataset_dict['test'][0]['image_path'] # Es solo filename
        example_text = "Una célula con núcleo redondo"
        similarity = calculate_similarity(model_fine_tuned, example_image_filename, example_text, IMAGE_BASE_PATH)
        print(f"\nEjemplo de Similitud entre la imagen '{example_image_filename}' y el texto '{example_text}': {similarity:.4f}")

    if wandb.run:
        wandb.finish()
    return model_fine_tuned

if __name__ == "__main__":
    try:
        if wandb.run: # Solo intentar desparchar si hay una ejecución activa
            wandb.tensorboard.unpatch()
    except AttributeError: # wandb.run es None si no se ha llamado a init
        pass
    except Exception as e: # Otras posibles excepciones de wandb
        print(f'Error al intentar wandb.tensorboard.unpatch(): {e}')

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Utilizando dispositivo: {device}")
    model = main()
